{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b10d2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /Users/carlossalasflores/opt/anaconda3/lib/python3.9/site-packages (0.6.1)\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f3d6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86f6774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.natural_language_processing.nlp_backbones import GPTSmall, GPTBase\n",
    "from src.models.computer_vision.backbones.vit import ViTBaseOver16at112, ViTBaseOver32at224, ViTSmallOver16at112, ViTMicroOver14at112\n",
    "from src.models.CLIP_model import CLIPModule\n",
    "from src.utils import load_from_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c8bfeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"data/imagenet/imagenet.csv\"\n",
    "image_path = \"data/imagenet/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a8716ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "def load_dataset(data_folder, filename, img_size=64):\n",
    "    data_file = os.path.join(data_folder, filename)\n",
    "\n",
    "    data_col = 'data'\n",
    "    label_col = 'labels'\n",
    "\n",
    "    d = unpickle(data_file)\n",
    "    x = d[data_col]\n",
    "    y = d[label_col]\n",
    "\n",
    "    img_size2 = img_size * img_size\n",
    "\n",
    "    x = np.dstack((x[:, :img_size2], x[:, img_size2:2*img_size2], x[:,2*img_size2:]))\n",
    "    x = x.reshape((len(x), img_size, img_size, 3)).transpose(0, 3, 1, 2)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def load_class_mapping(filename):\n",
    "    class_map = { num: None for num in range(1, 1001) }\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            _, idx, label = line.split()\n",
    "            class_map[int(idx)] = label\n",
    "            \n",
    "    return class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d32e5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_backbone(image_encoder, text_encoder, device):\n",
    "    image_model = None\n",
    "    image_resolution = None\n",
    "    \n",
    "    if image_encoder == \"B/32@224\":\n",
    "        image_model = ViTBaseOver32at224(dim_out=512).to(device)\n",
    "        image_resolution = 224\n",
    "    if image_encoder == \"B/16@112\":\n",
    "        image_model = ViTBaseOver16at112(dim_out=512).to(device)\n",
    "        image_resolution = 112\n",
    "    if image_encoder == \"S/16@112\":\n",
    "        image_model = ViTSmallOver16at112(dim_out=512).to(device)\n",
    "        image_resolution = 112\n",
    "    if image_encoder == \"M/14@112\":\n",
    "        image_model = ViTMicroOver14at112(dim_out=512).to(device)\n",
    "        image_resolution = 112\n",
    "\n",
    "    text_model = None\n",
    "    if text_encoder == \"S\":\n",
    "        text_model = GPTSmall(dim_out=768, vocab_size=43001, max_length=34, batch_size=18).to(device)\n",
    "    if text_encoder == \"B\":\n",
    "        text_model = GPTBase(dim_out=768, vocab_size=43001, max_length=34, batch_size=18).to(device)\n",
    "        \n",
    "    clip_model = CLIPModule(image_encoder=image_model, text_encoder=text_model, dim_img=512, dim_text=768, embedding_dim=512, temperature=0.07).to(device)\n",
    "    \n",
    "    return clip_model\n",
    "\n",
    "\n",
    "def tokenize(tokenizer, query, max_length):\n",
    "    # Encode sequence\n",
    "    encoded_query = tokenizer.encode(query).ids\n",
    "\n",
    "    # Truncate query if necessary\n",
    "    encoded_query = encoded_query[:max_length-2]\n",
    "\n",
    "    # Add end_of_sentence token [EOS]\n",
    "    encoded_query += [tokenizer.token_to_id('[EOS]')]\n",
    "\n",
    "    # Add padding to encoded sentence\n",
    "    encoded_query += [0] * (max_length - len(encoded_query) - 1)\n",
    "\n",
    "    # Add [SOS] and [EOS] tokens\n",
    "    encoded_query = [tokenizer.token_to_id('[SOS]')] + encoded_query\n",
    "    \n",
    "    return encoded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d76edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip(clip_model):\n",
    "    checkpointsdir = \"src/models/checkpoints\"\n",
    "    \n",
    "    if clip_model == \"ViT-Base/32 @ 224px\":\n",
    "        clip = load_clip_backbone(image_encoder=\"B/32@224\", text_encoder=\"B\", device=torch.device('cpu'))\n",
    "        _, loss_hist = clip.load_from_checkpoint(os.path.join(checkpointdir, \"CLIP_epoch_4_\"))\n",
    "        return clip\n",
    "        \n",
    "    if clip_model == \"ViT-Base/16 @ 112px\":\n",
    "        clip = load_clip_backbone(image_encoder=\"B/16@112\", text_encoder=\"B\", device=torch.device('cpu'))\n",
    "        _, loss_hist = clip.load_from_checkpoint(os.path.join(checkpointdir, \"CLIP_epoch_2_\"))\n",
    "        return clip\n",
    "    \n",
    "    if clip_model == \"ViT-Small/16 @ 112px\":\n",
    "        clip = load_clip_backbone(image_encoder=\"S/16@112\", text_encoder=\"B\", device=torch.device('cpu'))\n",
    "        _, loss_hist = clip.load_from_checkpoint(os.path.join(checkpointdir, \"CLIP_epoch_2_\"))\n",
    "        return clip\n",
    "        \n",
    "    if clip_model == \"ViT-Micro/14 @ 112px\":\n",
    "        clip = load_clip_backbone(image_encoder=\"M/14@112\", text_encoder=\"S\", device=torch.device('cpu'))\n",
    "        _, loss_hist = clip.load_from_checkpoint(os.path.join(checkpointdir, \"CLIP_epoch_2_\"))\n",
    "        return clip\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f8334",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4e43dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = f\"data/imagenet/\"\n",
    "tokenizer_file = \"src/data/nlp/tokenizers/CLIP-bpe.tokenizer.json\"\n",
    "\n",
    "imdir = os.path.join(datadir, \"images\")\n",
    "clabelsdir = os.path.join(datadir, \"map_clsloc.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66483a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_dataset(datadir, 'val_data', img_size=64)\n",
    "class_map = load_class_mapping(clabelsdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a810aa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val shape torch.Size([50000, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "X_val = torch.from_numpy(x)\n",
    "print(\"X_val shape\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441cba4",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ddc84c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211438848"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip = load_clip_backbone(image_encoder=\"B/32@224\", text_encoder=\"B\", device=torch.device('cpu'))\n",
    "np.sum([ np.prod(x.shape) for x in clip.parameters() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bbb710fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127588864"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip = load_clip_backbone(image_encoder=\"S/16@112\", text_encoder=\"B\", device=torch.device('cpu'))\n",
    "np.sum([ np.prod(x.shape) for x in clip.parameters() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "53d310ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6034315132099093"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "127588864 / 211438848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f83b650b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CLIPModule' object has no attribute 'load_from_checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j8/lbnsc2qj41jgyy60v7nltymc0000gn/T/ipykernel_3714/3867141357.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ViT-Base/32 @ 224px\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j8/lbnsc2qj41jgyy60v7nltymc0000gn/T/ipykernel_3714/2774277824.py\u001b[0m in \u001b[0;36mload_clip\u001b[0;34m(clip_model)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip_model\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ViT-Base/32 @ 224px\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_clip_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B/32@224\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpointdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CLIP_epoch_4_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1270\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CLIPModule' object has no attribute 'load_from_checkpoint'"
     ]
    }
   ],
   "source": [
    "clip = load_clip(\"ViT-Base/32 @ 224px\")\n",
    "len(clip.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f726bc6",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(tokenizer_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a9157",
   "metadata": {},
   "source": [
    "## Zero-shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0cdfb9",
   "metadata": {},
   "source": [
    "Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "547db527",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\n",
    "    'a photo of a {}.',\n",
    "    'a blurry photo of a {}.',\n",
    "    'a black and white photo of a {}.',\n",
    "    'a low contrast photo of a {}.',\n",
    "    'a high contrast photo of a {}.',\n",
    "    'a bad photo of a {}.',\n",
    "    'a good photo of a {}.',\n",
    "    'a photo of a small {}.',\n",
    "    'a photo of a big {}.',\n",
    "    'a photo of the {}.',\n",
    "    'a blurry photo of the {}.',\n",
    "    'a black and white photo of the {}.',\n",
    "    'a low contrast photo of the {}.',\n",
    "    'a high contrast photo of the {}.',\n",
    "    'a bad photo of the {}.',\n",
    "    'a good photo of the {}.',\n",
    "    'a photo of the small {}.',\n",
    "    'a photo of the big {}.',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28eece",
   "metadata": {},
   "source": [
    "### Compute zero-shot weights for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ecb5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_text = lambda x : tokenize(tokenizer, x.format(key), 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25516b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j8/lbnsc2qj41jgyy60v7nltymc0000gn/T/ipykernel_3714/2403277949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mzero_shot_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mclass_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemplates\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mzero_shot_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_shot_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j8/lbnsc2qj41jgyy60v7nltymc0000gn/T/ipykernel_3714/2403277949.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mzero_shot_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mclass_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemplates\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mzero_shot_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_shot_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j8/lbnsc2qj41jgyy60v7nltymc0000gn/T/ipykernel_3714/1593004911.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencode_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "zero_shot_weights = torch.zeros(1000, 768)\n",
    "for i, key in enumerate(class_map.keys()):\n",
    "    class_tokens = torch.from_numpy( np.array( [ encode_text(x) for x in templates ] ) )\n",
    "    zero_shot_weights[i, :] = clip.text_encoder(class_tokens).mean(dim=-1)\n",
    "print(zero_shot_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23faa7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
